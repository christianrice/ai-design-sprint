{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick prototype of 2 LLMs conversing with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_sprint_goal = \"Create a space laser for medical use\"\n",
    "expert = \"Josephine Yuen: Potion maker who has been growing and hand-grinding herbs for 30 years in Little Rock, Arkansas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_a.clear()\n",
    "history_b.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "REDIS_URL = \"redis://localhost:6379/0\"\n",
    "\n",
    "system_template_a = \"\"\"\n",
    "You are a member of a Design Sprint team who is tasked with interviewing an expert. Your job is to ask insightful questions of the expert, listen to their response, and then followup with another question every time they respond. However, you will only be able to ask 5 questions throughout the whole interview, and you must ask them one at a time. Tailor your questions to the expert's background so that you elicit as much interesting information from them as possible, and use interesting techniques to understand their current pain points. Never ask them questions about industries or experiences that seem like they are out of the expert's area of familiarity.\n",
    "\n",
    "Here is the goal of this design sprint, delimited below by triple backticks. You want to get information from th expert regarding this topic:\n",
    "\n",
    "```\n",
    "{design_sprint_goal}\n",
    "```\n",
    "\n",
    "And here is some background on the person you're talking to. Make sure you ask them interesting questions based upon their background and the information you learn from their responses:\n",
    "\n",
    "```\n",
    "{expert}\n",
    "```\n",
    "\n",
    "Now go ahead and begin the interview process. Remember, you are holding a dialogue with a real person. That means you should only ask one question at a time and wait for them to answer before you follow up with another question. This means each of your responses should be a SINGLE question, not a chain of questions. DO NOT ANSWER THE HUMAN RESPONSES ON YOUR OWN. Just ask a question. The format of your responses should ONLY contain a question, no other dialogue. If you do not follow these rules, you will be disqualified from the study and you will severely disappoint the researchers.\n",
    "\n",
    "Additionally, NEVER prepend your answer with \"AI: \"\n",
    "\"\"\"\n",
    "\n",
    "human_template_a = \"{question}\"\n",
    "\n",
    "\n",
    "prompt_a = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_template_a),\n",
    "        MessagesPlaceholder(variable_name=\"history_a\"),\n",
    "        HumanMessagePromptTemplate.from_template(human_template_a),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_a = Ollama(model=\"mistral\")\n",
    "\n",
    "chain_a = prompt_a | model_a\n",
    "\n",
    "history_a = RedisChatMessageHistory(session_id=\"converation_a\", url=REDIS_URL, ttl=600)\n",
    "\n",
    "chain_a_with_history = RunnableWithMessageHistory(\n",
    "    chain_a,\n",
    "    lambda session_id: history_a,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history_a\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_a = chain_a_with_history.invoke(\n",
    "    {\"design_sprint_goal\": design_sprint_goal, \"expert\": expert, \"question\": \"what would you like to ask me?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"converation_a\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_a = chain_a_with_history.invoke(\n",
    "    {\"design_sprint_goal\": \"How could we use space laser technology to make human life better?\", \"expert\": \"Potion maker\", \"question\": cleaned_response_b},\n",
    "    config={\"configurable\": {\"session_id\": \"converation_a\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_a.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "REDIS_URL = \"redis://localhost:6379/0\"\n",
    "\n",
    "system_template_b = \"\"\"\n",
    "You are adopting the following persona:\n",
    "\n",
    "```\n",
    "{expert}\n",
    "```\n",
    "\n",
    "You are being interviewed by a company who is exploring how to solve a problem around:\n",
    "\n",
    "```\n",
    "{design_sprint_goal}\n",
    "```\n",
    "\n",
    "Given this context, answer the questions they ask of you. You should really answer from your own deep, personal experience. Do not embellish or answer in ways that try to impress the interviewer. You should share your personal feelings according to your background, and you should not worry about pleasing the interviewer if your answers do not align with their problem solving mission. They are simply seeking honesty from you. Do not talk about industries outside your area of expertise, always come back to your persona to answer a question. \n",
    "\n",
    "Your answer should be about 150 words, and you should be detailed but concise.\n",
    "\n",
    "Additionally, ABSOLUTELY NEVER prepend your answer with \"AI: \", just answer the question. Otherwise, you will be disqualified from the study and you will severely disappoint the researchers.\n",
    "\"\"\"\n",
    "\n",
    "human_template_b = \"{question}\"\n",
    "\n",
    "\n",
    "prompt_b = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_template_b),\n",
    "        MessagesPlaceholder(variable_name=\"history_b\"),\n",
    "        HumanMessagePromptTemplate.from_template(human_template_b),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_b = Ollama(model=\"mistral\")\n",
    "\n",
    "chain_b = prompt_b | model_b\n",
    "\n",
    "history_b = RedisChatMessageHistory(session_id=\"converation_b\", url=REDIS_URL, ttl=600)\n",
    "\n",
    "chain_b_with_history = RunnableWithMessageHistory(\n",
    "    chain_b,\n",
    "    lambda session_id: history_b,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history_b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_b = chain_b_with_history.invoke(\n",
    "    {\"design_sprint_goal\": design_sprint_goal, \"expert\": expert, \"question\": response_a},\n",
    "    config={\"configurable\": {\"session_id\": \"converation_b\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_response_b = response_b.split(\"AI: \", 1)[-1]\n",
    "print(cleaned_response_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_b.messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
